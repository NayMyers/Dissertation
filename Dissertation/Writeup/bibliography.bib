@misc{,
title = {{(No Title)}},
url = {https://www.ifad.org/documents/38714170/39135645/smallholders_report.pdf/133e8903-0204-4e7d-a780-bca847933f2e},
urldate = {2021-02-04}
}
@misc{COMPUTERACCESS:1,
AUTHOR="Statista",
title = {{• How many people have access to a computer 2018 | Statista}},
url = {https://www.statista.com/statistics/748551/worldwide-households-with-computer/},
urldate = {2021-02-04},
year = {2021}
}
@misc{Statista:2021,
title = {{• Smartphone users 2020 | Statista}},
url = {https://www.statista.com/statistics/330695/number-of-smartphone-users-worldwide/},
urldate = {2021-02-04},
year = {2021}
}
@inproceedings{Anthonys2009,
abstract = {The classification and recognition of paddy diseases are of the major technical and economical importance in the agricultural industry. To automate these activities, like texture, color and shape, disease recognition system is feasible. The goal of this research is to develop an image recognition system that can recognize paddy diseases. Images were acquired under laboratory condition using digital camera. Three major diseases commonly found in Sri Lanka, Rice blast (Magnaporthe grisea), Rice sheath blight (Rhizoctonia solani) and Brown spot (Cochiobolus miyabeanus] were selected for this research. Image processing starts with the digitized a color image of paddy disease leaf. Then a method of mathematics morphology is used to segment these images. Then texture, shape and color features of color image of disease spot on leaf were extracted, and a classification method of membership function was used to discriminate between the three types of diseases. The analysis of the results showed over 70 percent classification accuracy around 50 sample images. {\textcopyright}2009 IEEE.},
author = {Anthonys, G. and Wickramarachchi, N.},
booktitle = {ICIIS 2009 - 4th International Conference on Industrial and Information Systems 2009, Conference Proceedings},
doi = {10.1109/ICIINFS.2009.5429828},
isbn = {9781424448371},
keywords = {CIE L*a*b* color space,Color texture,Mathematics morphology,Membership function},
pages = {403--407},
title = {{An image recognition system for crop disease identification of paddy fields in Sri Lanka}},
year = {2009}
}
@misc{Globaltt,
title = {{ArabSat 5C - Internet by Satellite in Africa}},
url = {https://www.globaltt.com/en/coverages-Arabsat 5C_C.html},
urldate = {2021-02-04},
year = {2021}
}
@misc{Google,
title = {{best vegetables to grow - Explore - Google Trends}},
url = {https://trends.google.com/trends/explore?q=best vegetables to grow&date=all&geo=US},
urldate = {2021-02-04},
year = {2021}
}
@misc{ImarcGroup,
title = {{Digital Camera Market Share, Size, Trends and Forecast 2021-2026}},
url = {https://www.imarcgroup.com/digital-camera-market},
urldate = {2021-02-04},
year = {2021}
}
@techreport{JLIFADSmallHolders,
author = {Walpole, M., Smith, J., Rosser, A., Brown, C., Schulte-Herbruggen, B., Booth, H., Sassen, M., Mapendembe, A., Fancourt, M., Bieri, M., Glaser, S., Corrigan, C., Narloch, U., Runsten, L., Jenkins, M., Gomera, M. and Hutton, J. },
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2013 - Enabling poor rural people to overcome poverty Smallholders, food security, and the environment.pdf:pdf},
title = {{Enabling poor rural people to overcome poverty Smallholders, food security, and the environment}},
url = {https://www.ifad.org/documents/38714170/39135645/smallholders_report.pdf/133e8903-0204-4e7d-a780-bca847933f2e},
year = {2013}
}
@misc{Wikipedia,
title = {{File:Internet users per 100 inhabitants ITU.svg - Wikipedia}},
url = {https://en.wikipedia.org/wiki/File:Internet_users_per_100_inhabitants_ITU.svg},
urldate = {2021-02-04},
year = {2021}
}
@article{Couper2018,
abstract = {Challenges to survey data collection have increased the costs of social research via face-to-face surveys so much that it may become extremely difficult for social scientists to continue using these methods. A key drawback to less expensive Internet-based alternatives is the threat of biased results from coverage errors in survey data. The rise of Internet-enabled smartphones presents an opportunity to re-examine the issue of Internet coverage for surveys and its implications for coverage bias. Two questions (on Internet access and smartphone ownership) were added to the National Survey of Family Growth (NSFG), a U.S. national probability survey of women and men age 15–44, using a continuous sample design. We examine 16 quarters (4 years) of data, from September 2012 to August 2016. Overall, we estimate that 82.9% of the target NSFG population has Internet access, and 81.6% has a smartphone. Combined, this means that about 90.7% of U.S. residents age 15–44 have Internet access, via either traditional devices or a smartphone. We find some evidence of compensatory coverage when looking at key race/ethnicity and age subgroups. For instance, while Black teens (15–18) have the lowest estimated rate of Internet access (81.9%) and the lowest rate of smartphone usage (72.6%), an estimated 88.0% of this subgroup has some form of Internet access. We also examine the socio-demographic correlates of Internet and smartphone coverage, separately and combined, as indicators of technology access in this population. In addition, we look at the effect of differential coverage on key estimates produced by the NSFG, related to fertility, family formation, and sexual activity. While this does not address nonresponse or measurement biases that may differ for alternative modes, our paper has implications for possible coverage biases that may arise when switching to a Web-based mode of data collection, either for follow-up surveys or to replace the main face-to-face data collection.},
author = {Couper, Mick P. and Gremel, Garret and Axinn, William and Guyer, Heidi and Wagner, James and West, Brady T.},
doi = {10.1016/j.ssresearch.2018.03.008},
issn = {0049089X},
journal = {Social Science Research},
keywords = {Coverage bias,Internet,Smartphone,Survey data},
month = {jul},
pages = {221--235},
pmid = {29793688},
publisher = {Academic Press Inc.},
title = {{New options for national population surveys: The implications of internet and smartphone coverage}},
volume = {73},
year = {2018}
}
@article{Affairs2013,
abstract = {The System Usability Scale (SUS) is a reliable tool for measuring the usability.   It consists of a 10 item questionnaire with five response options for respondents; from Strongly agree to Strongly disagree.},
author = {Affairs, Assistant Secretary for Public},
keywords = {UX,User experience,sus,sus questionnaire,system usability scale,usability,usability testing},
month = {sep},
publisher = {Department of Health and Human Services},
title = {{System Usability Scale (SUS)}},
year = {2013}
}
@misc{,
title = {{System Usability Scale (SUS) | Usability.gov}},
url = {https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html},
urldate = {2021-02-04},
year = {2021}
}
@misc{,
title = {{tensorflow/lucid: A collection of infrastructure and tools for research in neural network interpretability.}},
url = {https://github.com/tensorflow/lucid},
urldate = {2021-02-04},
year = {2021}
}
@article{Mohanty2016,
abstract = {Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.},
author = {Mohanty, Sharada P. and Hughes, David P. and Salath{\'{e}}, Marcel},
doi = {10.3389/fpls.2016.01419},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohanty, Hughes, Salath{\'{e}} - 2016 - Using Deep Learning for Image-Based Plant Disease Detection.pdf:pdf},
issn = {1664-462X},
journal = {Frontiers in Plant Science},
keywords = {Crop diseases,Deep learning,Digital epidemiology,Machine learning},
month = {sep},
number = {September},
pages = {1419},
publisher = {Frontiers Research Foundation},
title = {{Using Deep Learning for Image-Based Plant Disease Detection}},
url = {http://journal.frontiersin.org/article/10.3389/fpls.2016.01419/full},
volume = {7},
year = {2016}
}
@misc{System_Usability2020,
title = {{System Usability Scale (SUS) | Usability.gov}},
url = {https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html},
urldate = {2021-02-04},
year = {2021}
}
@techreport{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - Unknown - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/},
year = {2012}
}
@InProceedings{Szegedy_2015_CVPR,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
title = {Going Deeper With Convolutions},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}
@misc{Yandex,
url = {https://yandex.com/images/},
year = {2021}
}
@misc{,
title = {{(No Title)}},
url = {https://softwaretestinggenius.com/docs/4aa5-7619.pdf},
urldate = {2021-03-02}
}
@misc{Highsmith2001,
author = {Highsmith, Jim and Cockburn, Alistair},
booktitle = {Computer},
doi = {10.1109/2.947100},
issn = {00189162},
month = {sep},
number = {9},
pages = {120--122},
title = {{Agile software development: The business of innovation}},
volume = {34},
year = {2001}
}
@article{Larsson2016,
abstract = {We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.},
archivePrefix = {arXiv},
arxivId = {1605.07648},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
eprint = {1605.07648},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Larsson, Maire, Shakhnarovich - 2016 - FractalNet Ultra-Deep Neural Networks without Residuals.pdf:pdf},
month = {may},
title = {{FractalNet: Ultra-Deep Neural Networks without Residuals}},
url = {http://arxiv.org/abs/1605.07648},
year = {2016}
}
@article{Wu2019,
abstract = {The community has been going deeper and deeper in designing one cutting edge network after another, yet some works are there suggesting that we may have gone too far in this dimension. Some researchers unravelled a residual network into an exponentially wider one, and assorted the success of residual networks to fusing a large amount of relatively shallow models. Since some of their early claims are still not settled, we in this paper dig more on this topic, i.e., the unravelled view of residual networks. Based on that, we try to find a good compromise between the depth and width. Afterwards, we walk through a typical pipeline of developing a deep-learning-based algorithm. We start from a group of relatively shallow networks, which perform as well or even better than the current (much deeper) state-of-the-art models on the ImageNet classification dataset. Then, we initialize fully convolutional networks (FCNs) using our pre-trained models, and tune them for semantic image segmentation. Results show that the proposed networks, as pre-trained features, can boost existing methods a lot. Even without exhausting the sophistical techniques to improve the classic FCN model, we achieve comparable results with the best performers on four widely-used datasets, i.e., Cityscapes, PASCAL VOC, ADE20k and PASCAL-Context. The code and pre-trained models are released for public access 1 .},
archivePrefix = {arXiv},
arxivId = {1611.10080},
author = {Wu, Zifeng and Shen, Chunhua and van den Hengel, Anton},
doi = {10.1016/j.patcog.2019.01.006},
eprint = {1611.10080},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Image classification,Residual network,Semantic segmentation},
month = {jun},
pages = {119--133},
publisher = {Elsevier Ltd},
title = {{Wider or Deeper: Revisiting the ResNet Model for Visual Recognition}},
volume = {90},
year = {2019}
}
@article{Zhu2018,
abstract = {Plant identification is a critical step in protecting plant diversity. However, many existing identification systems prohibitively rely on hand-crafted features for plant species identification. In this paper, a deep learning method is employed to extract discriminative features from plant images along with a linear SVM for plant identification. To offer a self-learning feature representation for different plant organs, we choose a very deep convolutional neural networks (CNNs), which consists of sixteen convolutional layers followed by three Fully-Connected (FC) layers and a final soft-max layer. Five max-pooling layers are performed over a 2×2 pixel window with stride 2. Extensive experiments on several plant datasets demonstrate the remarkable performance of the very deep neural network compared to the hand-crafted features.},
author = {Zhu, Heyan and Liu, Qinglin and Qi, Yuankai and Huang, Xinyuan and Jiang, Feng and Zhang, Shengping},
doi = {10.1007/s11042-017-5578-9},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2018 - Plant identification based on very deep convolutional neural networks.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {CNN,Linear SVM,Plant identification},
month = {nov},
number = {22},
pages = {29779--29797},
publisher = {Springer New York LLC},
title = {{Plant identification based on very deep convolutional neural networks}},
url = {https://doi.org/10.1007/s11042-017-5578-9},
volume = {77},
year = {2018}
}
@techreport{Choi,
abstract = {This paper describes our participation at the LifeCLEF Plant identification task 2015. Given various images of plant parts such as leaf, flower or stem, this task is about identification of plant species given multi-image observation query. We utilized GoogLeNet for individual image classification, and combined image classification results for plant identification per observation. Our approach achieved best performance in this task.},
author = {Choi, Sungbin},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi - Unknown - Plant identification with deep convolutional neural network SNUMedinfo at LifeCLEF plant identification task 2015.pdf:pdf},
keywords = {Borda-fuse,Deep convolutional neural network,Goog-LeNet,Image classification},
title = {{Plant identification with deep convolutional neural network: SNUMedinfo at LifeCLEF plant identification task 2015}}
}
@article{Mohanty2016,
abstract = {Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.},
author = {Mohanty, Sharada P. and Hughes, David P. and Salath{\'{e}}, Marcel},
doi = {10.3389/fpls.2016.01419},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohanty, Hughes, Salath{\'{e}} - 2016 - Using Deep Learning for Image-Based Plant Disease Detection(3).pdf:pdf},
issn = {1664-462X},
journal = {Frontiers in Plant Science},
keywords = {Crop diseases,Deep learning,Digital epidemiology,Machine learning},
month = {sep},
number = {September},
pages = {1419},
publisher = {Frontiers Research Foundation},
title = {{Using Deep Learning for Image-Based Plant Disease Detection}},
url = {http://journal.frontiersin.org/article/10.3389/fpls.2016.01419/full},
volume = {7},
year = {2016}
}
@inproceedings{Khirade2015,
abstract = {Identification of the plant diseases is the key to preventing the losses in the yield and quantity of the agricultural product. The studies of the plant diseases mean the studies of visually observable patterns seen on the plant. Health monitoring and disease detection on plant is very critical for sustainable agriculture. It is very difficult to monitor the plant diseases manually. It requires tremendous amount of work, expertize in the plant diseases, and also require the excessive processing time. Hence, image processing is used for the detection of plant diseases. Disease detection involves the steps like image acquisition, image pre-processing, image segmentation, feature extraction and classification. This paper discussed the methods used for the detection of plant diseases using their leaves images. This paper also discussed some segmentation and feature extraction algorithm used in the plant disease detection.},
author = {Khirade, Sachin D. and Patil, A. B.},
booktitle = {Proceedings - 1st International Conference on Computing, Communication, Control and Automation, ICCUBEA 2015},
doi = {10.1109/ICCUBEA.2015.153},
isbn = {9781479968923},
keywords = {Feature extraction,Image acquisition,Segmentation},
month = {jul},
pages = {768--771},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Plant disease detection using image processing}},
year = {2015}
}
@article{Ashqar2018,
author = {Ashqar, Belal A . M. and Abu-Naser, Samy S.},
keywords = {Article,Deep Learning,Detection,Diseases,Tomato Leaves},
publisher = {IJARW},
title = {{Image-Based Tomato Leaves Diseases Detection Using Deep Learning}},
url = {http://dspace.alazhar.edu.ps/xmlui/handle/123456789/278},
year = {2018}
}
@article{Mohanty2016a,
abstract = {Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.},
author = {Mohanty, Sharada P. and Hughes, David P. and Salath{\'{e}}, Marcel},
doi = {10.3389/fpls.2016.01419},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohanty, Hughes, Salath{\'{e}} - 2016 - Using Deep Learning for Image-Based Plant Disease Detection(2).pdf:pdf},
issn = {1664-462X},
journal = {Frontiers in Plant Science},
keywords = {Crop diseases,Deep learning,Digital epidemiology,Machine learning},
month = {sep},
number = {September},
pages = {1419},
publisher = {Frontiers Research Foundation},
title = {{Using Deep Learning for Image-Based Plant Disease Detection}},
url = {http://journal.frontiersin.org/article/10.3389/fpls.2016.01419/full},
volume = {7},
year = {2016}
}
@inproceedings{Kulkarni2018,
abstract = {In recent times, drastic climate changes and lack of immunity in crops has caused substantial increase in growth of crop diseases. This causes large scale demolition of crops, decreases cultivation and eventually leads to financial loss of farmers. Due to rapid growth in variety of diseases and adequate knowledge of farmer, identification and treatment of the disease has become a major challenge. The leaves have texture and visual similarities which attributes for identification of disease type. Hence, computer vision employed with deep learning provides the way to solve this problem. This paper proposes a deep learning-based model which is trained using public dataset containing images of healthy and diseased crop leaves. The model serves its objective by classifying images of leaves into diseased category based on the pattern of defect.},
author = {Kulkarni, Omkar},
booktitle = {Proceedings - 2018 4th International Conference on Computing, Communication Control and Automation, ICCUBEA 2018},
doi = {10.1109/ICCUBEA.2018.8697390},
isbn = {9781538652572},
keywords = {Crop disease,Deep learning,Image classification,InceptionV3,MobileNet,Transfer Learning},
month = {jul},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Crop Disease Detection Using Deep Learning}},
year = {2018}
}
@misc{,
title = {{IEEE Xplore Full-Text PDF:}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8697390},
urldate = {2021-02-26}
}
@techreport{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the Im-ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in-carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Going Deeper with Convolutions.pdf:pdf},
pages = {1--9},
title = {{Going Deeper with Convolutions}},
year = {2015}
}
@techreport{Krizhevsky,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - Unknown - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/}
}
@misc{,
title = {{IEEE Xplore Full-Text PDF:}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8697390},
urldate = {2021-02-07}
}
@techreport{Jaware2012,
abstract = {The K-Means clustering technique is a well-known approach that has been applied to solve low-level image segmentation tasks. This clustering algorithm is convergent and its aim is to optimize the partitioning decisions based on a user-defined initial set of clusters that is updated after each iteration. In the first step we identify the mostly green colored pixels. Next, these pixels are masked based on specific threshold values that are computed using Otsu's method, then those mostly green pixels are masked. The other additional step is that the pixels with zeros red, green and blue values and the pixels on the boundaries of the infected cluster (object) were completely removed. The experimental results demonstrate that the proposed technique is a robust technique for the detection of plant leaves diseases.},
author = {Jaware, Tushar H and Badgujar, Ravindra D and Patil, G and Prof, Asst},
booktitle = {World Journal of Science and Technology},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaware et al. - 2012 - Held at R.C.Patel Institute of Technology.pdf:pdf},
keywords = {Crop diseases,Image Segmentation,K-Means clustering},
number = {4},
pages = {190--194},
title = {{Held at R.C.Patel Institute of Technology}},
url = {www.worldjournalofscience.com},
volume = {2012},
year = {2012}
}
@article{Affairs2013,
abstract = {The System Usability Scale (SUS) is a reliable tool for measuring the usability.   It consists of a 10 item questionnaire with five response options for respondents; from Strongly agree to Strongly disagree.},
author = {Affairs, Assistant Secretary for Public},
keywords = {UX,User experience,sus,sus questionnaire,system usability scale,usability,usability testing},
month = {sep},
publisher = {Department of Health and Human Services},
title = {{System Usability Scale (SUS)}},
year = {2013}
}
@misc{,
title = {{System Usability Scale (SUS) | Usability.gov}},
url = {https://www.usability.gov/how-to-and-tools/methods/system-usability-scale.html},
urldate = {2021-02-04}
}
@misc{,
title = {{(No Title)}},
url = {https://www.ifad.org/documents/38714170/39135645/smallholders_report.pdf/133e8903-0204-4e7d-a780-bca847933f2e},
urldate = {2021-02-04}
}
@techreport{,
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2013 - Enabling poor rural people to overcome poverty Smallholders, food security, and the environment.pdf:pdf},
title = {{Enabling poor rural people to overcome poverty Smallholders, food security, and the environment}},
year = {2013}
}
@misc{,
title = {{tensorflow/lucid: A collection of infrastructure and tools for research in neural network interpretability.}},
url = {https://github.com/tensorflow/lucid},
urldate = {2021-02-04}
}
@misc{,
title = {{• Smartphone users 2020 | Statista}},
url = {https://www.statista.com/statistics/330695/number-of-smartphone-users-worldwide/},
urldate = {2021-02-04}
}
@article{Mohanty2016,
abstract = {Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.},
author = {Mohanty, Sharada P. and Hughes, David P. and Salath{\'{e}}, Marcel},
doi = {10.3389/fpls.2016.01419},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohanty, Hughes, Salath{\'{e}} - 2016 - Using Deep Learning for Image-Based Plant Disease Detection.pdf:pdf},
issn = {1664-462X},
journal = {Frontiers in Plant Science},
keywords = {Crop diseases,Deep learning,Digital epidemiology,Machine learning},
month = {sep},
number = {September},
pages = {1419},
publisher = {Frontiers Research Foundation},
title = {{Using Deep Learning for Image-Based Plant Disease Detection}},
url = {http://journal.frontiersin.org/article/10.3389/fpls.2016.01419/full},
volume = {7},
year = {2016}
}
@misc{,
title = {{File:Internet users per 100 inhabitants ITU.svg - Wikipedia}},
url = {https://en.wikipedia.org/wiki/File:Internet_users_per_100_inhabitants_ITU.svg},
urldate = {2021-02-04}
}
@misc{,
title = {{• How many people have access to a computer 2018 | Statista}},
url = {https://www.statista.com/statistics/748551/worldwide-households-with-computer/},
urldate = {2021-02-04}
}
@misc{,
title = {{best vegetables to grow - Explore - Google Trends}},
url = {https://trends.google.com/trends/explore?q=best vegetables to grow&date=all&geo=US},
urldate = {2021-02-04}
}
@misc{,
title = {{Digital Camera Market Share, Size, Trends and Forecast 2021-2026}},
url = {https://www.imarcgroup.com/digital-camera-market},
urldate = {2021-02-04}
}
@article{Couper2018,
abstract = {Challenges to survey data collection have increased the costs of social research via face-to-face surveys so much that it may become extremely difficult for social scientists to continue using these methods. A key drawback to less expensive Internet-based alternatives is the threat of biased results from coverage errors in survey data. The rise of Internet-enabled smartphones presents an opportunity to re-examine the issue of Internet coverage for surveys and its implications for coverage bias. Two questions (on Internet access and smartphone ownership) were added to the National Survey of Family Growth (NSFG), a U.S. national probability survey of women and men age 15–44, using a continuous sample design. We examine 16 quarters (4 years) of data, from September 2012 to August 2016. Overall, we estimate that 82.9% of the target NSFG population has Internet access, and 81.6% has a smartphone. Combined, this means that about 90.7% of U.S. residents age 15–44 have Internet access, via either traditional devices or a smartphone. We find some evidence of compensatory coverage when looking at key race/ethnicity and age subgroups. For instance, while Black teens (15–18) have the lowest estimated rate of Internet access (81.9%) and the lowest rate of smartphone usage (72.6%), an estimated 88.0% of this subgroup has some form of Internet access. We also examine the socio-demographic correlates of Internet and smartphone coverage, separately and combined, as indicators of technology access in this population. In addition, we look at the effect of differential coverage on key estimates produced by the NSFG, related to fertility, family formation, and sexual activity. While this does not address nonresponse or measurement biases that may differ for alternative modes, our paper has implications for possible coverage biases that may arise when switching to a Web-based mode of data collection, either for follow-up surveys or to replace the main face-to-face data collection.},
author = {Couper, Mick P. and Gremel, Garret and Axinn, William and Guyer, Heidi and Wagner, James and West, Brady T.},
doi = {10.1016/j.ssresearch.2018.03.008},
issn = {0049089X},
journal = {Social Science Research},
keywords = {Coverage bias,Internet,Smartphone,Survey data},
month = {jul},
pages = {221--235},
pmid = {29793688},
publisher = {Academic Press Inc.},
title = {{New options for national population surveys: The implications of internet and smartphone coverage}},
volume = {73},
year = {2018}
}
@misc{,
title = {{ArabSat 5C - Internet by Satellite in Africa}},
url = {https://www.globaltt.com/en/coverages-Arabsat 5C_C.html},
urldate = {2021-02-04}
}
@inproceedings{Anthonys2009,
abstract = {The classification and recognition of paddy diseases are of the major technical and economical importance in the agricultural industry. To automate these activities, like texture, color and shape, disease recognition system is feasible. The goal of this research is to develop an image recognition system that can recognize paddy diseases. Images were acquired under laboratory condition using digital camera. Three major diseases commonly found in Sri Lanka, Rice blast (Magnaporthe grisea), Rice sheath blight (Rhizoctonia solani) and Brown spot (Cochiobolus miyabeanus] were selected for this research. Image processing starts with the digitized a color image of paddy disease leaf. Then a method of mathematics morphology is used to segment these images. Then texture, shape and color features of color image of disease spot on leaf were extracted, and a classification method of membership function was used to discriminate between the three types of diseases. The analysis of the results showed over 70 percent classification accuracy around 50 sample images. {\textcopyright}2009 IEEE.},
author = {Anthonys, G. and Wickramarachchi, N.},
booktitle = {ICIIS 2009 - 4th International Conference on Industrial and Information Systems 2009, Conference Proceedings},
doi = {10.1109/ICIINFS.2009.5429828},
isbn = {9781424448371},
keywords = {CIE L*a*b* color space,Color texture,Mathematics morphology,Membership function},
pages = {403--407},
title = {{An image recognition system for crop disease identification of paddy fields in Sri Lanka}},
year = {2009}
}
@techreport{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisa-tion and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556v6},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf:pdf},
keywords = {()},
title = {{VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION}},
url = {http://www.robots.ox.ac.uk/},
year = {2015}
}
@techreport{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@techreport{Zagoruyko,
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
archivePrefix = {arXiv},
arxivId = {1605.07146v4},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
eprint = {1605.07146v4},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zagoruyko, Komodakis - Unknown - Wide Residual Networks.pdf:pdf},
title = {{Wide Residual Networks}}
}
@misc{,
title = {{OpenAI Microscope}},
url = {https://microscope.openai.com/models},
urldate = {2021-03-06}
}
@techreport{He,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers-8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions 1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet local-ization, COCO detection, and COCO segmentation.},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://image-net.org/challenges/LSVRC/2015/}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Cun1989,
author = {Cun, Y. Le and Guyon, I. and Jackel, L. D. and Henderson, D. and Boser, B. and Howard, R. E. and Denker, J. S. and Hubbard, W. and Graf, H. P.},
doi = {10.1109/35.41400},
issn = {01636804},
journal = {IEEE Communications Magazine},
mendeley-groups = {dis_LitReview},
number = {11},
pages = {41--46},
title = {{Handwritten Digit Recognition: Applications of Neural Network Chips and Automatic Learning}},
volume = {27},
year = {1989}
}
@misc{CunYannle1988,
author = {{Cun Yann le}},
mendeley-groups = {dis_LitReview},
title = {{A Theoretical Framework for Back-Propagation}},
url = {http://new.math.uiuc.edu/MathMLseminar/seminarPapers/LeCunBackprop1988.pdf},
year = {1988}
}
@article{Sermanet2013,
abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
archivePrefix = {arXiv},
arxivId = {1312.6229},
author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
eprint = {1312.6229},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sermanet et al. - 2013 - OverFeat Integrated Recognition, Localization and Detection using Convolutional Networks.pdf:pdf},
journal = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
keywords = {()},
mendeley-groups = {dis_LitReview},
month = {dec},
publisher = {International Conference on Learning Representations, ICLR},
title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
url = {http://arxiv.org/abs/1312.6229},
year = {2013}
}
@techreport{StudiengangBachelor,
abstract = {The area of web development has changed a lot in the past years. This thesis provides an insight into the currently most popular JavaScript frameworks: Angular, React and Vue.js. Each one of them will be investigated and evaluated based on pre-deened criteria. Ultimately, a recommendation will be given on which technology is most appropriate for certain situations.},
author = {im {Studiengang Bachelor} and Pr{\"{u}}ferin, Betreuende and {Steeens Zweitgutachter}, Ulrike and {Behrmann geb Knoblauch}, Martin and Wohlgethan, Eric},
file = {:C\:/Users/Ryan-Syme/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Studiengang Bachelor et al. - Unknown - Bachelorarbeit eingereicht im Rahmen der Bachelorpr{\"{u}}fung.pdf:pdf},
keywords = {Angular,JavaScript,React,Vuejs,frontend,web development},
mendeley-groups = {dis_Main},
title = {{Bachelorarbeit eingereicht im Rahmen der Bachelorpr{\"{u}}fung}}
}
@book{hunt2000pragmatic,
  added-at = {2013-11-29T20:55:50.000+0100},
  address = {Boston [etc.]},
  author = {Hunt, Andrew and Thomas, David},
  biburl = {https://www.bibsonomy.org/bibtex/2f089d7f6f61cad0258c08477d2f920a7/admogar},
  description = {The Pragmatic Programmer: From Journeyman to Master: Andrew Hunt, David Thomas: 9780201616224: Amazon.com: Books},
  interhash = {e614934d54a2ffbddf23a42d0dc729ff},
  intrahash = {f089d7f6f61cad0258c08477d2f920a7},
  isbn = {020161622X 9780201616224},
  keywords = {programming software},
  publisher = {Addison-Wesley},
  refid = {806497391},
  timestamp = {2013-12-02T09:48:03.000+0100},
  title = {The Pragmatic programmer : from journeyman to master},
  url = {http://www.amazon.com/The-Pragmatic-Programmer-Journeyman-Master/dp/020161622X},
  year = 2000
}
@book{G.Polya1945,
 ISBN = {9780691164076},
 URL = {http://www.jstor.org/stable/j.ctvc773pk},
 abstract = { A perennial bestseller by eminent mathematician G. Polya, How to Solve It will show anyone in any field how to think straight. In lucid and appealing prose, Polya reveals how the mathematical method of demonstrating a proof or finding an unknown can be of help in attacking any problem that can be "reasoned" out-from building a bridge to winning a game of anagrams. Generations of readers have relished Polya's deft-indeed, brilliant-instructions on stripping away irrelevancies and going straight to the heart of the problem. },
 author = {G. POLYA},
 publisher = {Princeton University Press},
 title = {How to Solve It: A New Aspect of Mathematical Method},
 year = {1945}
}
@article{Halevy2009,
author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
doi = {10.1109/MIS.2009.36},
issn = {15411672},
journal = {IEEE Intelligent Systems},
number = {2},
pages = {8--12},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{The unreasonable effectiveness of data}},
volume = {24},
year = {2009}
}
@book{Clegg199410.5555/561543,
author = {Clegg, Dai and Barker, Richard},
title = {Case Method Fast-Track: A Rad Approach},
year = {1994},
isbn = {020162432X},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA},
abstract = {From the Book: In writing this book I tried to achieve two different objectives. The first objective was to provide an overview of fast-track, the techniques it applies, and particularly the management challenges it presents. The second objective was to provide a handbook for project managers running fast-track projects. Putting these together might have endangered the second, and more pragmatic objective. This I did not want to do, so the book is split into two distinct parts. Chapters 1 to 3 deal with fast-track in general, Chapter 4 to 7 deal with the life-cycle of a project in more detail, identifying the tasks and their deliverable, the techniques and the tools that support them. These later chapters refer back to the relevant sections of the first part of the book so the descriptions of specific techniques are all gathered together in Chapter 3 and not scattered and duplicated through subsequent chapters. I hope that this structure will mean a casual reader can gain an understanding of the fast-track approach quickly from the first part of the book, and the practitioner can have an organized and compact reference for daily use from the second part. In assembling the second part I frequently found myself adding an extra step or an additional task, for completeness. The result is that, while not all projects will need every step of every task, they are all necessary under some circumstances. Every task is included in a project on the basis of what it produces. If a particular project has no need of the result, for example, documentation of test results, then it should not be produced and the task should be omitted. During the initial planning of a project the template project plan offeredin Chapters 4 to 6 should be assessed and tailored for the project, cutting out the unnecessary tasks and steps. Do not be afraid to prune. The questions to ask are: "Do I need this deliverable to manage the project " and "Does the sponsor value this deliverable above its price " I have used Oracle tools throughout in examples, and in particular I have recommended an approach that exploits the ability of the Oracle CASE tools to define rules once then reuse them many times. Some CASE tools do not exploit reusability in this way, but the trend in development technology is towards greater reusability so it seems very appropriate here. Dai CleggJuly 1994}
}
@misc{Highsmith2001,
author = {Highsmith, Jim and Cockburn, Alistair},
booktitle = {Computer},
doi = {10.1109/2.947100},
issn = {00189162},
month = {sep},
number = {9},
pages = {120--122},
title = {{Agile software development: The business of innovation}},
volume = {34},
year = {2001}
}
@book{Gall1977,
author = {John Gall},
booktitle = {Systemantics},
pages = {71},
year = {1977}
}
Gall, J., 1977. Systemantics. [Place of publication not identified]: Wildwood House, p.71.
